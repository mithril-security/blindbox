{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7-tQfVM_pJcr"
   },
   "source": [
    "<h1>Integrating OpenAI's Whisper model with BlindBox</h1>\n",
    "______________________________\n",
    "\n",
    "## Introduction\n",
    "______________________________\n",
    "\n",
    "In this tutorial, we're going to walk through how we created a Whisper application image that can be deployed with BlindBox. We will cover two key steps:\n",
    "\n",
    "1. How we created a BlindBox-compatible API for the OpenAI Whisper model using [FastAPI](https://fastapi.tiangolo.com/).\n",
    "2. How we created our Docker image for our API.\n",
    "\n",
    "You can see how we deploy the image with BlindBox in the [quick-tour](../getting-started/quick-tour.ipynb).\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "____________________\n",
    "\n",
    "To follow along with this tutorial, you will need to:\n",
    "\n",
    "+ Have Docker installed in your environment. Here's the [Docker installation guide](https://docs.docker.com/desktop/install/linux-install/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Whisper FastAPI\n",
    "_______________________\n",
    "\n",
    "Our first task in deploying the Whisper OpenAI model with BlindBox was to create an API so that our end users will be able to query the model. We did this using the FastAPI library which allows us to quickly assign functions to API endpoints.\n",
    " \n",
    "The full code we use to do this is available in the `server.py` file in the `ai_server_example` folder on BlindBox's official GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mithril-security/blindbox\n",
    "!cd ai_server_example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key sections in this code:\n",
    "\n",
    "### Initial set-up\n",
    "\n",
    "Firstly, we load the OpenAI tiny English Whisper model from Hugging Face, as well as initializing out API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Some settings\n",
    "STT = \"openai/whisper-tiny.en\" # Model name (HuggingFace)\n",
    "MODEL_STORE_ADDR = \"172.17.0.1\" # Address of the model store\n",
    "\n",
    "# Initialize our FastAPI API object\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model and tokenizer\n",
    "whisper_processor = load_from_store(STT, WhisperProcessor, MODEL_STORE_ADDR)\n",
    "whisper_model = load_from_store(STT, WhisperForConditionalGeneration, MODEL_STORE_ADDR)\n",
    "whisper_model.eval()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a predict endpoint\n",
    "\n",
    "Secondly, we create a `/whisper/predict` POST endpoint on our FastAPI application object. This endpoint will convert the audio file to a tensor and then query the model with our input data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# This is a POST endpoint located at /whisper/predict\n",
    "@app.post(\"/whisper/predict\")\n",
    "@async_speech_to_text_endpoint(sample_rate=16000) # We use the async_speech_to_text_endpoint to handle conversion from an audio file to a tensor\n",
    "\n",
    "async def predict(x: np.ndarray) -> str:\n",
    "    \n",
    "    input_features = whisper_processor(\n",
    "        x, sampling_rate=16000, return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # We query the model through the runner\n",
    "    predicted_ids = await whisper_runner.submit(input_features)\n",
    "    \n",
    "    # We decode our results\n",
    "    transcription = whisper_processor.batch_decode(\n",
    "        predicted_ids, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return transcription[0]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a runner for querying the model which uses adaptive batching and a separate thread to avoid blocking the event loop since this process is quite intensive.\n",
    "\n",
    "The runner is launched when the server starts up and executes the `run_whisper` function when a batch is ready to be processed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# A function that wraps prediction code and that will be executed by the runner\n",
    "def run_whisper(x: torch.Tensor) -> torch.Tensor:\n",
    "    return whisper_model.generate(x, max_length=128)\n",
    "\n",
    "\n",
    "# Define a runner (i.e. the given function will be run on a separate thread with adaptive batching)\n",
    "whisper_runner = BatchRunner(\n",
    "    run_whisper,\n",
    "    max_batch_size=256,\n",
    "    max_latency_ms=200,\n",
    "    collator=TorchCollator(),\n",
    ")\n",
    "\n",
    "app.on_event(\"startup\")(whisper_runner.run) # Schedule the runner to run when the server is up\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching our server\n",
    "\n",
    "Finally, we deploys our API on a python ASGI `uvicorn` server (an asynchronous web server) on `port 80`. It is essential to use port 80 as BlindBox will need to be able to communicate with our application on this port!\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=80)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, we packaged the Whisper model as an API by doing the following:\n",
    "\n",
    "+ Creating an API app object that \"configures\" the `uvicorn` server by providing handlers for specific endpoints\n",
    "\n",
    "+ Creating a `whisper/predict` endpoint which in turn queries the Whisper model.\n",
    "\n",
    "+ Deploy our API on our server on `port 80`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging our application in a Docker image\n",
    "________________________________\n",
    "\n",
    "Once we had created out Whisper API, all that was left to do was create a **Docker image** for our application that could then be deployed in BlindBox. Let's take a look at the Dockerfile we used to do this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```docker\n",
    "FROM python:3.10.10-bullseye as base\n",
    "\n",
    "# install necessary dependencies\n",
    "RUN pip install \\\n",
    "    torch==1.13.1 \\\n",
    "    transformers==4.26.1 \\\n",
    "    fastapi==0.95.0 \\\n",
    "    python-multipart==0.0.6 \\\n",
    "    uvicorn==0.21.1 \\\n",
    "    soundfile==0.12.1 \\\n",
    "    messages \\\n",
    "    librosa==0.10.0 \\\n",
    "    pydantic==1.10.7 \\\n",
    "    requests==2.28.2 \\\n",
    "    --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "COPY batch_runner.py /\n",
    "COPY collators.py /\n",
    "COPY messages.py /\n",
    "COPY model_store.py /\n",
    "COPY serializers.py /\n",
    "COPY server.py /\n",
    "\n",
    "# signal our app runs on port 80\n",
    "EXPOSE 80\n",
    "\n",
    "# launch our server\n",
    "CMD [\"python server.py\"]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Same as for the application code, this file can be viewed in the `ai_server_example` folder on the official BlindBox GitHub repository.\n",
    "\n",
    "There are no complex requirements for the Docker image, but it is recommended to `EXPOSE` port 80 to signal that the application will be running on port 80 within our BlindBox."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "_______________________\n",
    "\n",
    "The Whisper app is now ready to be built and deployed on BlindBox!\n",
    "\n",
    "You can see exactly how we do this in our [Quick Tour](../getting-started/quick-tour.ipynb).\n",
    " \n",
    "In this tutorial, we've seen how we can:\n",
    "+ Create a BlindBox-compatible application\n",
    "+ Create an application image for our application, ready to be built and deployed on BlindBox!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "blindbox-preview-7Yaoi9am-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
